{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN & PCA"
      ],
      "metadata": {
        "id": "YkiPm7MEsOlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1.** What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "\n",
        " - K-Nearest Neighbors (KNN) is one of the simplest and most intuitive supervised machine learning algorithms used for both classification and regression.\n",
        "It belongs to the family of instance-based or lazy learning algorithms because it does not explicitly learn a model during training — instead, it stores all the training data and makes predictions only when asked.\n",
        "\n",
        " - KNN in Classification:\n",
        "\n",
        "1. Choose a value of K (number of neighbors).\n",
        "\n",
        "2. Calculate the distance between the query point and all training data points.\n",
        "\n",
        "3. Select the K nearest neighbors based on smallest distances.\n",
        "\n",
        "4. Count the class labels of these K neighbors.\n",
        "\n",
        "5. Assign the query point to the majority class among the neighbors.\n",
        "\n",
        "   - (Optional: use distance-weighted voting → closer neighbors have more influence).\n",
        "\n",
        "- KNN in Regression:\n",
        "\n",
        "1. Choose a value of K.\n",
        "\n",
        "2. Calculate the distance between the query point and all training data points.\n",
        "\n",
        "3. Select the K nearest neighbors based on smallest distances.\n",
        "\n",
        "4. Take the average (or weighted average) of the neighbors’ target values.\n",
        "\n",
        "5. Assign this average as the predicted value for the query point.\n",
        "\n",
        "**Q2.**  What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        " - The curse of dimensionality refers to the challenges that arise when analyzing data in very high-dimensional spaces. As the number of features (dimensions) increases, data points become sparse and the concept of distance loses its effectiveness. In K-Nearest Neighbors (KNN), which relies on measuring distances to find the nearest neighbors, this leads to difficulties because in high dimensions, the difference between the nearest and farthest neighbors becomes negligible. As a result, KNN struggles to identify truly similar points, making its predictions less accurate and more computationally expensive.\n",
        "\n",
        "- How It Affects KNN Performance:\n",
        "\n",
        "1. Distances lose significance:\n",
        "\n",
        "   - In high dimensions, the distance between the closest and farthest neighbors tends to become almost the same.\n",
        "\n",
        "   - This makes it hard for KNN to identify truly “nearest” neighbors.\n",
        "\n",
        "2. Increased computation:\n",
        "\n",
        "   - More dimensions = more features to compute distance → KNN becomes computationally heavy.\n",
        "\n",
        "3. Overfitting risk\n",
        "\n",
        "   - With too many features, KNN may fit to noise rather than useful patterns.\n",
        "\n",
        "   - Unless features are reduced or selected carefully, performance degrades.\n",
        "\n",
        "4. Need for feature scaling & selection\n",
        "\n",
        "   - Some features may dominate the distance calculation, leading to biased predictions.\n",
        "\n",
        "    - KNN becomes very sensitive to irrelevant or redundant features in high-dimensional space.\n",
        "\n",
        "**Q3.** What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "- Principal Component Analysis (PCA) is a dimensionality reduction technique used to simplify large datasets by transforming the original features into a new set of uncorrelated features called principal components. These components are ordered in such a way that the first few retain most of the variation (information) present in the original data. Unlike feature selection, PCA does not discard features but instead creates new ones that are linear combinations of the original variables, making the dataset smaller while still capturing its essential patterns.\n",
        "\n",
        "- Difference Between PCA and Feature Selection:\n",
        "\n",
        "1. Nature of Features\n",
        "\n",
        "   - PCA: Creates new features (principal components).\n",
        "\n",
        "   - Feature Selection: Keeps original features.\n",
        "\n",
        "2. Method\n",
        "\n",
        "   - PCA: Transformation-based (projects data into a new space).\n",
        "\n",
        "   - Feature Selection: Selection-based (chooses best subset of features).\n",
        "\n",
        "3. Interpretability\n",
        "\n",
        "   - PCA: Components are often less interpretable.\n",
        "\n",
        "   - Feature Selection: Features are easy to interpret.\n",
        "\n",
        "4. Goal\n",
        "\n",
        "   - PCA: Preserve maximum variance in fewer dimensions.\n",
        "\n",
        "   - Feature Selection: Keep only most relevant features for prediction.\n",
        "\n",
        "5. Output\n",
        "\n",
        "   - PCA: Produces a new dataset with reduced dimensions.\n",
        "\n",
        "   - Feature Selection: Produces a smaller version of the original dataset.\n",
        "\n",
        "**Q4.**   What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "- Eigenvectors\n",
        "\n",
        "    - In PCA, eigenvectors represent the directions (axes) in the feature space along which the data varies the most.\n",
        "\n",
        "   - Each eigenvector corresponds to a principal component.\n",
        "\n",
        "   - They define the “new axes” after PCA transformation.\n",
        "\n",
        "- Eigenvalues\n",
        "\n",
        "   - Eigenvalues tell us the amount of variance captured by each eigenvector (principal component).\n",
        "\n",
        "   - A higher eigenvalue means that component captures more information (variance) from the data.\n",
        "\n",
        "- Why They Are Important in PCA\n",
        "\n",
        "1. Identify Principal Components\n",
        "\n",
        "   - Eigenvectors determine the orientation of the new feature space (principal components).\n",
        "\n",
        "2. Measure Variance (Importance)\n",
        "\n",
        "   - Eigenvalues tell us how much variance each principal component explains.\n",
        "\n",
        "   - Example: If the first eigenvalue is very large, then the first principal component explains most of the data variance.\n",
        "\n",
        "3. Dimensionality Reduction\n",
        "\n",
        "   - By sorting eigenvalues in descending order and keeping only the top ones, PCA reduces the dataset to fewer dimensions while still preserving most of the information.\n",
        "\n",
        "**Q5.** How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "- How KNN and PCA Complement Each Other\n",
        "\n",
        "1. PCA reduces dimensionality\n",
        "\n",
        "   - KNN struggles in high-dimensional spaces because of the curse of dimensionality (distances lose meaning).\n",
        "\n",
        "   - PCA projects data onto fewer dimensions (principal components) while preserving most variance.\n",
        "\n",
        "   - This makes distance calculations in KNN more reliable.\n",
        "\n",
        "2. Improved Efficiency\n",
        "\n",
        "   - KNN is computationally expensive since it compares distances to all training points.\n",
        "\n",
        "   - With PCA, fewer dimensions → fewer distance calculations → faster KNN predictions.\n",
        "\n",
        "3. Noise Reduction\n",
        "\n",
        "   - PCA removes less important components (low variance features = often noise).\n",
        "\n",
        "   - This helps KNN focus only on the most informative features, improving accuracy.\n",
        "\n",
        "4. Better Generalization\n",
        "\n",
        "   - By reducing overfitting caused by irrelevant features, PCA helps KNN generalize better on unseen data.\n",
        "   "
      ],
      "metadata": {
        "id": "mt9EcRBKsUWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnfJreGCsMy9",
        "outputId": "3a869fef-bcfb-41dd-b550-b6345840847f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7407407407407407\n",
            "Accuracy with scaling   : 0.9629629629629629\n"
          ]
        }
      ],
      "source": [
        "#Q6.  Train a KNN Classifier on the Wine dataset with and without feature\n",
        "# scaling. Compare model accuracy in both cases.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. KNN WITHOUT SCALING\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "acc_no_scaling = knn_no_scaling.score(X_test, y_test)\n",
        "\n",
        "# 2. KNN WITH SCALING\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_with_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_with_scaling.fit(X_train_scaled, y_train)\n",
        "acc_with_scaling = knn_with_scaling.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with scaling   :\", acc_with_scaling)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, var in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {var:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPJSKp0Zzqzz",
        "outputId": "b35a9bb4-a689-402b-c79f-fe6289e511fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "acc_original = knn_original.score(X_test_scaled, y_test)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_pca = knn_pca.score(X_test_pca, y_test)\n",
        "\n",
        "print(\"Accuracy on Original Scaled Data:\", acc_original)\n",
        "print(\"Accuracy on PCA (2 components)  :\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYtg58qZ0Vo9",
        "outputId": "02a30b96-3f72-401e-c2f7-97596bd27d53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Original Scaled Data: 0.9629629629629629\n",
            "Accuracy on PCA (2 components)  : 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "acc_euclidean = knn_euclidean.score(X_test_scaled, y_test)\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "acc_manhattan = knn_manhattan.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"KNN Accuracy with Euclidean distance:\", acc_euclidean)\n",
        "print(\"KNN Accuracy with Manhattan distance:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfH-2nty0zCZ",
        "outputId": "cb77b904-8ed0-4ac1-c51b-126471009ec5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy with Euclidean distance: 0.9629629629629629\n",
            "KNN Accuracy with Manhattan distance: 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q10.** You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "\n",
        "Explain how you would:\n",
        "\n",
        "● Use PCA to reduce dimensionality\n",
        "\n",
        "● Decide how many components to keep\n",
        "\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "\n",
        "● Evaluate the model\n",
        "\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "\n",
        " - We are dealing with a gene expression dataset that has thousands of features (genes) but only a small number of patient samples. This imbalance makes traditional models prone to overfitting, since they try to learn from too many features with too little data. To address this, I would build a pipeline using PCA + KNN, explained below:\n",
        "\n",
        "\n",
        "#### 1. **Using PCA to Reduce Dimensionality**\n",
        "\n",
        "* Gene expression data is typically very high-dimensional, and many genes may carry redundant or noisy information.\n",
        "* I would apply Principal Component Analysis (PCA) to transform the data into a smaller number of new features (principal components).\n",
        "* These components capture the most important patterns (variance) in the dataset, while filtering out noise.\n",
        "* This step makes the dataset more compact and less prone to overfitting.\n",
        "\n",
        "\n",
        "#### 2. **Deciding How Many Components to Keep**\n",
        "\n",
        "* After applying PCA, I would check the explained variance ratio of each component.\n",
        "* I’d then create a cumulative variance plot (scree plot) to see how much total variance is explained as we add more components.\n",
        "* A common practice is to keep enough components to explain around 90–95% of the variance, balancing information retention and dimensionality reduction.\n",
        "* This ensures we don’t lose important biological signals while still reducing noise.\n",
        "\n",
        "\n",
        "#### 3. **Using KNN for Classification**\n",
        "\n",
        "* Once the data is reduced using PCA, I would train a K-Nearest Neighbors (KNN) classifier on the transformed dataset.\n",
        "* KNN is a simple, non-parametric algorithm that works well when combined with PCA because distances in the reduced space become more meaningful.\n",
        "* I would tune the hyperparameter K (number of neighbors) using cross-validation to find the best value for classification performance.\n",
        "\n",
        "\n",
        "#### 4. **Evaluating the Model**\n",
        "\n",
        "* To evaluate the model, I would use **stratified cross-validation**, ensuring class proportions are preserved (important in biomedical data with imbalanced classes).\n",
        "* Metrics I would focus on include:\n",
        "\n",
        "  * Accuracy (overall performance).\n",
        "  * Precision, Recall, and F1-score (important for medical diagnosis to avoid false negatives).\n",
        "  * Confusion Matrix (to see misclassification patterns between cancer types).\n",
        "\n",
        "\n",
        "#### 5. **Justifying the Pipeline to Stakeholders**\n",
        "\n",
        "* Why PCA? It reduces thousands of noisy gene features into a smaller, informative set, preventing overfitting and speeding up training.\n",
        "* Why KNN? It is interpretable, requires no assumptions about data distribution, and works well in the PCA-reduced space.\n",
        "* Why is this robust? This pipeline reduces complexity, improves generalization to unseen patients, and provides reproducible results. In biomedical research, simpler and transparent models are often more trusted than black-box models.\n",
        "* Real-world relevance: Many genomic studies use PCA for dimensionality reduction, and combining it with a straightforward classifier like KNN ensures a balance between accuracy and interpretability — which is critical for medical decision-making.\n",
        "\n",
        "- **In summary**:\n",
        "The pipeline **PCA → KNN → Evaluation** provides a practical and reliable way to classify cancer patients using gene expression data, while controlling overfitting and ensuring the results are both accurate and interpretable for real-world biomedical use.\n"
      ],
      "metadata": {
        "id": "7Hd61orh2Nkz"
      }
    }
  ]
}